{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import credit_card_data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "\n",
    "Z,Y = credit_card_data.generate_raw_data(60, 10000)\n",
    "train_data, train_label, test_data, test_label = credit_card_data.generate_win_data(Z, Y, window_size)\n",
    "train_data = torch.tensor(train_data, dtype=torch.float, device=device)\n",
    "train_label = torch.tensor(train_label, dtype=torch.float, device=device)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float, device=device)\n",
    "test_label = torch.tensor(test_label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52521])\n",
      "tensor(3398., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_label.size())\n",
    "print(train_label.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for z in Z:\n",
    "    i += int(z[10] != 2)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for z in Z:\n",
    "    i += int(z[59] != 2)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, output_dim=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = 1\n",
    "        self.hidden_dim = 3\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = 2\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # (\"input size: \", input.size())\n",
    "        lstm_out, hidden = self.lstm(input.view(input.size()[0],input.size()[1],-1))\n",
    "        y_pred = self.linear(lstm_out[-1])\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "num_train = len(train_data)\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTM(batch_size=batch_size, output_dim=output_dim)\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = torch.nn.BCELoss(size_average=False)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(num_epochs):\n",
    "    for i in range(len(train_data)//batch_size):\n",
    "        optimiser.zero_grad()\n",
    "        batch_data = train_data[i*batch_size:(i+1)*batch_size].permute(1,0)\n",
    "        batch_label = train_label[i*batch_size:(i+1)*batch_size]\n",
    "        y_pred = model(batch_data)\n",
    "        weight = 10 * (batch_label == 1).float() + torch.ones(batch_size, device=device)\n",
    "        loss = torch.nn.BCELoss(weight=weight)(y_pred, batch_label)\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def criteria(y_test, test_label):\n",
    "    recall = []\n",
    "    precision = []\n",
    "    for t in range(102):\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        threshold = t*0.01 - 0.01\n",
    "        TP += ((y_test > threshold).numpy().flatten()&(test_label == 1).numpy()).sum()\n",
    "        TN += ((y_test < threshold).numpy().flatten()&(test_label == 0).numpy()).sum()\n",
    "        FP += ((y_test > threshold).numpy().flatten()&(test_label == 0).numpy()).sum()\n",
    "        FN += ((y_test < threshold).numpy().flatten()&(test_label == 1).numpy()).sum()\n",
    "        recall.append((TP) / (TP + FN))\n",
    "        precision.append((TP) / (TP + FP))\n",
    "        print(recall[-1], precision[-1])\n",
    "    return recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "1.0 0.06655331552101044\n",
      "0.7652068126520681 0.10765017970220778\n",
      "0.7469586374695864 0.11728748806112703\n",
      "0.7384428223844283 0.12242839854780153\n",
      "0.7311435523114356 0.12320623206232062\n",
      "0.7116788321167883 0.12706342311033883\n",
      "0.7068126520681265 0.12839779005524862\n",
      "0.7055961070559611 0.12851761577664525\n",
      "0.6922141119221411 0.13226406322640633\n",
      "0.6909975669099757 0.13462905901872482\n",
      "0.6897810218978102 0.13682432432432431\n",
      "0.6897810218978102 0.13705583756345177\n",
      "0.6897810218978102 0.1374878758486906\n",
      "0.6885644768856448 0.1381161542215715\n",
      "0.6800486618004866 0.1440721649484536\n",
      "0.6763990267639902 0.14608512874408827\n",
      "0.6678832116788321 0.14746172441579372\n",
      "0.6678832116788321 0.14793856103476152\n",
      "0.6678832116788321 0.14817813765182186\n",
      "0.6666666666666666 0.14859002169197397\n",
      "0.6666666666666666 0.14879174585935379\n",
      "0.6618004866180048 0.14912280701754385\n",
      "0.6618004866180048 0.14932747735382926\n",
      "0.6338199513381995 0.1563156315631563\n",
      "0.6277372262773723 0.15833077631175208\n",
      "0.6240875912408759 0.15951492537313433\n",
      "0.6240875912408759 0.16021236727045596\n",
      "0.6240875912408759 0.16036261331666146\n",
      "0.6240875912408759 0.16116870876531575\n",
      "0.621654501216545 0.16145339652448656\n",
      "0.621654501216545 0.16211928934010153\n",
      "0.6180048661800487 0.16334405144694533\n",
      "0.6167883211678832 0.163495646565624\n",
      "0.6046228710462287 0.16644340254521098\n",
      "0.5851581508515815 0.16800558854348585\n",
      "0.5851581508515815 0.16800558854348585\n",
      "0.5474452554744526 0.1756440281030445\n",
      "0.5413625304136253 0.17686804451510335\n",
      "0.5401459854014599 0.17809867629362214\n",
      "0.537712895377129 0.17880258899676377\n",
      "0.5304136253041363 0.1783960720130933\n",
      "0.5218978102189781 0.17972350230414746\n",
      "0.48175182481751827 0.18705715635333017\n",
      "0.4245742092457421 0.18514588859416445\n",
      "0.4245742092457421 0.18514588859416445\n",
      "0.4245742092457421 0.18514588859416445\n",
      "0.4245742092457421 0.18514588859416445\n",
      "0.30900243309002434 0.19613899613899613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n",
      "0.0 nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test = model(test_data.permute(1,0)).cpu()\n",
    "recall, precision = criteria(y_test, test_label)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2341, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7196, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss_fn):\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(len(train_data)//batch_size):\n",
    "            model.zero_grad()\n",
    "            y_pred = model(train_data[i*batch_size:(i+1)*batch_size])\n",
    "            loss = loss_fn(y_pred, train_label[i*batch_size:(i+1)*batch_size])\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (512) != input nelement (10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f4cade3029fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-3867d444e5f1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loss_fn)\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-e30641795dab>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mBCE_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mBCE_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mBCE_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mF_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBCE_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[1;32m-> 2058\u001b[1;33m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (512) != input nelement (10)"
     ]
    }
   ],
   "source": [
    "loss_2 = FocalLoss()\n",
    "train(loss_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
