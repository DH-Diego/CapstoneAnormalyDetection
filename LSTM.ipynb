{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import credit_card_data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "\n",
    "Z,Y = credit_card_data.generate_raw_data(36,50000)\n",
    "train_data, train_label, test_data, test_label = credit_card_data.generate_win_data(Z, Y, window_size)\n",
    "train_data = torch.tensor(train_data, dtype=torch.float)\n",
    "train_label = torch.tensor(train_label, dtype=torch.float)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float)\n",
    "test_label = torch.tensor(test_label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=3, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        print(input.view(input.size()[0],1,input.size()[1]).size())\n",
    "        lstm_out, hidden = self.lstm(input.view(input.size()[0],1,input.size()[1]))\n",
    "        print(lstm_out.size())\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'batch_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-611e6c593de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_input_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'batch_first'"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "lstm_input_size = window_size\n",
    "h1 = 3\n",
    "num_train = len(train_data)\n",
    "output_dim = 1\n",
    "num_layers = 1\n",
    "model = LSTM(lstm_input_size, h1, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.005\n",
    "loss_fn = torch.nn.BCELoss(size_average=False)\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([263536])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = np.concatenate([train_data,train_label.unsqueeze(1)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(loss_fn):\n",
    "    model = LSTM(lstm_input_size, h1, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "    model.cuda()\n",
    "    for t in range(num_epochs):\n",
    "        train = np.concatenate([train_data,train_label.unsqueeze(1)],axis = 1)\n",
    "        shuffle(train)\n",
    "        train_data_ = torch.tensor(train[:,:12])\n",
    "        train_label_ = torch.tensor(train[:,12])\n",
    "        for i in range(len(train_data_)//batch_size):\n",
    "            \n",
    "            model.zero_grad()\n",
    "            y_pred = model(train_data_[i*batch_size:(i+1)*batch_size].to(device))\n",
    "            loss = loss_fn(y_pred, train_label_[i*batch_size:(i+1)*batch_size].to(device))\n",
    "            if i % 100 == 0:\n",
    "                print('epoch: '+str(t)+'\\tbatch: '+str(i)+'\\tloss: ',loss.item())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_2 = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tbatch: 0\tloss:  0.046885982155799866\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "epoch: 0\tbatch: 100\tloss:  0.046160049736499786\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "epoch: 0\tbatch: 200\tloss:  0.04637591913342476\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])\n",
      "torch.Size([256, 1, 3])\n",
      "torch.Size([256, 1, 12])"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-188-e12e9fa67a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-186-2950a179b9c3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loss_fn)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-182-cd81d41ec79d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misatty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = train(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\tbatch: 1\tloss:  0.8911981582641602\n",
      "epoch: 0\tbatch: 251\tloss:  0.8985716700553894\n",
      "epoch: 0\tbatch: 501\tloss:  0.8964217901229858\n",
      "epoch: 0\tbatch: 751\tloss:  0.8964589834213257\n",
      "epoch: 0\tbatch: 1001\tloss:  0.8985828757286072\n",
      "epoch: 1\tbatch: 1\tloss:  0.8867354989051819\n",
      "epoch: 1\tbatch: 251\tloss:  0.8905588388442993\n",
      "epoch: 1\tbatch: 501\tloss:  0.8960592150688171\n",
      "epoch: 1\tbatch: 751\tloss:  0.8970898985862732\n",
      "epoch: 1\tbatch: 1001\tloss:  0.9004513025283813\n",
      "epoch: 2\tbatch: 1\tloss:  0.893153190612793\n",
      "epoch: 2\tbatch: 251\tloss:  0.8917361497879028\n",
      "epoch: 2\tbatch: 501\tloss:  0.8914210796356201\n",
      "epoch: 2\tbatch: 751\tloss:  0.8941531181335449\n",
      "epoch: 2\tbatch: 1001\tloss:  0.8943278789520264\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(lstm_input_size, h1, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "model.to(device)\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "for t in range(num_epochs):\n",
    "    train = np.concatenate([train_data,train_label.unsqueeze(1)],axis = 1)\n",
    "    shuffle(train)\n",
    "    train_data_ = torch.tensor(train[:,:12])\n",
    "    train_label_ = torch.tensor(train[:,12])\n",
    "    for i in range(len(train_data_)//batch_size):\n",
    "#         model.zero_grad()\n",
    "        y_pred = model(train_data_[i*batch_size:(i+1)*batch_size].to(device))\n",
    "        weight = np.ones(batch_size)\n",
    "        for j in range(len(train_label_[i*batch_size:(i+1)*batch_size])):\n",
    "            if train_label_[i*batch_size:(i+1)*batch_size][j] == 1:\n",
    "                weight[j] = 2\n",
    "        weight = torch.tensor(weight,dtype = torch.float).to(device)\n",
    "        loss = torch.nn.BCELoss(weight=weight)(y_pred, train_label_[i*batch_size:(i+1)*batch_size].to(device))\n",
    "        if i % 250 == 1:\n",
    "            print('epoch: '+str(t)+'\\tbatch: '+str(i)+'\\tloss: ',loss.item())\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "recall = []\n",
    "precision = []\n",
    "y_test = model(test_data.to(device)).cpu()\n",
    "for t in range(1000):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    threshold = t*0.001\n",
    "    TP += ((y_test > threshold).numpy().reshape(1,-1).flatten()&(test_label == 1).numpy()).sum()\n",
    "    TN += ((y_test < threshold).numpy().reshape(1,-1).flatten()&(test_label == 0).numpy()).sum()\n",
    "    FP += ((y_test > threshold).numpy().reshape(1,-1).flatten()&(test_label == 0).numpy()).sum()\n",
    "    FN += ((y_test < threshold).numpy().reshape(1,-1).flatten()&(test_label == 1).numpy()).sum()\n",
    "    recall.append((TP) / (TP + FN))\n",
    "    precision.append((TP) / (TP + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(recall,precision)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
